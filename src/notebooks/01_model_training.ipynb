{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5341a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721b3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Create Dummy Data (Expanded)\n",
    "\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This is an amazing product!\",\n",
    "        \"I hate this, it's terrible.\",\n",
    "        \"The service was mediocre.\",\n",
    "        \"I'm feeling happy and great.\",\n",
    "        \"This is the worst experience ever.\",\n",
    "        \"It's alright, not bad.\",\n",
    "        \"Absolutely fantastic!\",\n",
    "        \"Complete waste of money.\",\n",
    "        \"I love this new feature!\",\n",
    "        \"This is so frustrating and useless.\",\n",
    "        \"The customer support was very helpful.\",\n",
    "        \"It's an okay product, nothing special.\",\n",
    "        \"I will never buy this again.\",\n",
    "        \"This is the best! Highly recommend.\",\n",
    "        \"Such a disappointment.\",\n",
    "        \"I'm very satisfied with the purchase.\"\n",
    "    ],\n",
    "    'sentiment': [1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1]  # 1 = Positive, 0 = Negative\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data (now 16 rows total)\n",
    "# test_size=0.25 will give us 12 for training and 4 for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[['text']], df['sentiment'], test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea705b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Phase 1: Model Training and MLflow Integration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\GitHub\\giga-flow-mlops\\.venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "2025/11/12 09:06:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Shape of X_train: (12, 1)\n",
      "DEBUG: Shape of y_train: (12,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\GitHub\\giga-flow-mlops\\.venv\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- MLflow Run Complete ---\n",
      "Run ID: 8a728fdad39840148eb0412b4b351b21\n",
      "Logged Metrics: Accuracy=0.7500, F1-Score=0.6429\n",
      "Model artifact logged to 'model' directory within the run.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Phase 1.1 & 1.2 - Train Model and Log with MLflow (Corrected)\n",
    "\n",
    "# --- NEW IMPORT ---\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# ------------------\n",
    "\n",
    "print(\"Starting Phase 1: Model Training and MLflow Integration...\")\n",
    "mlflow.set_experiment(\"GigaFlow-Sentiment\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # --- 1. Model Training ---\n",
    "    \n",
    "    params = {\n",
    "        \"C\": 1.0,\n",
    "        \"solver\": \"liblinear\",\n",
    "        \"random_state\": 42\n",
    "    }\n",
    "\n",
    "    # --- DEFINE THE PIPELINE WITH COLUMNTRANSFORMER ---\n",
    "    \n",
    "    # This preprocessor tells the pipeline to apply TfidfVectorizer \n",
    "    # ONLY to the 'text' column and to pass other columns through.\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('tfidf', TfidfVectorizer(), 'text')  # (name, transformer, column_name)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # Create the full pipeline\n",
    "    model_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('clf', LogisticRegression(C=params['C'], solver=params['solver'], random_state=params['random_state']))\n",
    "    ])\n",
    "    \n",
    "    # --- END OF NEW PIPELINE DEFINITION ---\n",
    "    \n",
    "    # (Optional) Debug prints - you can keep or remove these\n",
    "    print(f\"DEBUG: Shape of X_train: {X_train.shape}\")\n",
    "    print(f\"DEBUG: Shape of y_train: {y_train.shape}\")\n",
    "    \n",
    "    # Train the model (X_train is still the df[['text']] from Cell 2)\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # --- 2. MLflow Integration (Logging) ---\n",
    "    \n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression_with_TFIDF\")\n",
    "\n",
    "    # Make predictions and log metrics (X_test is the df[['text']] from Cell 2)\n",
    "    preds = model_pipeline.predict(X_test)\n",
    "    \n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    f1 = f1_score(y_test, preds, average='weighted')\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    \n",
    "    # Log the final model artifact\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model_pipeline,\n",
    "        artifact_path=\"model\",\n",
    "        # X_train.iloc[:1] is now a (1, 1) DataFrame, which is perfect.\n",
    "        input_example=X_train.iloc[:1] \n",
    "    )\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"\\n--- MLflow Run Complete ---\")\n",
    "    print(f\"Run ID: {run_id}\")\n",
    "    print(f\"Logged Metrics: Accuracy={acc:.4f}, F1-Score={f1:.4f}\")\n",
    "    print(f\"Model artifact logged to 'model' directory within the run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4f9a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Phase 1.3: Local Testing ---\n",
      "Loading model from Run ID: 8a728fdad39840148eb0412b4b351b21\n",
      "DEBUG: Predictions array: [0 0]\n",
      "\n",
      "--- Inference Results ---\n",
      "Input: This is a truly wonderful and amazing product -> Prediction: Negative\n",
      "Input: I am so angry and frustrated with this. -> Prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Phase 1.3 - Local Testing \n",
    "print(f\"\\n--- Starting Phase 1.3: Local Testing ---\")\n",
    "print(f\"Loading model from Run ID: {run_id}\")\n",
    "\n",
    "# 1. Define the URI to load the model\n",
    "logged_model_uri = f\"runs:/{run_id}/model\"\n",
    "\n",
    "# 2. Load the model as a 'pyfunc'\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model_uri)\n",
    "\n",
    "# 3. Test with new data\n",
    "test_data = pd.DataFrame({\n",
    "    'text': [\n",
    "        \"This is a truly wonderful and amazing product\",\n",
    "        \"I am so angry and frustrated with this.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "predictions = loaded_model.predict(test_data)\n",
    "\n",
    "print(f\"DEBUG: Predictions array: {predictions}\")\n",
    "\n",
    "print(\"\\n--- Inference Results ---\")\n",
    "print(f\"Input: {test_data['text'].iloc[0]} -> Prediction: {'Positive' if predictions[0] == 1 else 'Negative'}\")\n",
    "print(f\"Input: {test_data['text'].iloc[1]} -> Prediction: {'Positive' if predictions[1] == 1 else 'Negative'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
